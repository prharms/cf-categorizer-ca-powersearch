# Campaign Finance Contributor Categorization Tool

This tool categorizes campaign contributors using Claude 4 Sonnet AI to analyze contributor names, employers, and occupations. **Now with enhanced architecture, parallel processing, comprehensive error handling, and modern Python packaging!**

Specifically designed for **California campaign finance data** exported from the Secretary of State's PowerSearch portal.

## Key Features

- **PowerSearch Integration**: Optimized for California Secretary of State campaign finance exports
- **AI-Powered Categorization**: Uses Claude 4 Sonnet for intelligent contributor classification
- **Two-Stage Pipeline**: Raw AI categorization + fuzzy matching standardization
- **Sequential Processing**: Reliable, rate-limit-friendly processing with progress tracking
- **Progress Persistence**: Resume processing if interrupted - no need to start over
- **Comprehensive Logging**: Detailed logs with configurable levels for debugging and monitoring
- **Enhanced Error Handling**: Robust validation and graceful error recovery
- **Comprehensive Testing**: 95%+ test coverage with automated test runner

## Input Data Requirements

### Data Source
This tool is designed specifically for CSV files exported from the **California Secretary of State's PowerSearch** portal:
[California Campaign Contribution Search](https://powersearch.sos.ca.gov/advanced.php)

### Required Pre-processing Steps
**Critical**: After downloading from PowerSearch, you must:
1. Open the CSV file in a text editor or Excel
2. **Remove all non-contribution lines from the bottom** of the file (summary statistics, footers, report metadata, etc.)
3. Ensure the file contains only the header row and actual contribution data rows
4. Save the cleaned file in your `data/raw/` directory

### Required CSV Columns
The PowerSearch export must contain these exact column names:
- `Contributor Name` - Name of the contributor (individual, committee, or organization)
- `Contributor Employer` - Employer information (may be blank)
- `Contributor Occupation` - Occupation information (may be blank)

### File Requirements
- **Format**: CSV (Comma Separated Values)
- **Encoding**: UTF-8 preferred (avoid Excel's default encoding issues)
- **Size**: No hard limit, but files >100MB may take 1-2 hours to process
- **Date Range**: Any date range supported by PowerSearch (2001-present)

## Quick Start (5 Minutes)

### Step 1: Get Your Campaign Finance Data
1. Visit [PowerSearch Advanced Search](https://powersearch.sos.ca.gov/advanced.php)
2. Select your search criteria:
   - Choose date range or election cycle
   - Select contributors (all or specific)
   - Choose recipients (candidates, committees, ballot measures)
3. Export results as CSV
4. **Clean the CSV**: Open in text editor and remove footer lines after the last contribution row

### Step 2: Set Up the Tool
```bash
git clone https://github.com/prharms/cf-categorizer-ca-powersearch.git
cd cf-categorizer-ca-powersearch
pip install -e .
echo "ANTHROPIC_API_KEY=your_key_here" > .env
```

**Legacy alternative**: If you prefer the traditional approach:
```bash
pip install -r requirements.txt
```

### Step 3: Categorize Your Data
```bash
# Place your cleaned PowerSearch CSV in data/raw/
python scripts/run_categorization.py data/raw/your_powersearch_export.csv
```

### Step 4: Review Results
- **Raw AI categories**: `data/interim/your_file_categorized.csv`
- **Final standardized**: `data/processed/your_file_standardized.csv`

## Examples

### Sample PowerSearch Export (Before Cleaning)
```csv
Contributor Name,Contributor Employer,Contributor Occupation,Amount,Date,Recipient Name
"John Doe","Smith & Associates","Attorney",1000.00,"2023-03-15","Jane Smith for Assembly"
"California Teachers Association","","",5000.00,"2023-03-20","Education PAC"
"Dr. Sarah Wilson","Kaiser Permanente","Physician",500.00,"2023-04-01","Healthcare Initiative"
"ACME Development LLC","","",2500.00,"2023-04-05","Housing Committee"

Total Contributions: $9,000.00
Export Date: 2024-01-15
Report Generated by PowerSearch
Search Criteria: Assembly candidates, 2023-2024 cycle
```

### After Manual Cleanup (Ready for Processing)
```csv
Contributor Name,Contributor Employer,Contributor Occupation,Amount,Date,Recipient Name
"John Doe","Smith & Associates","Attorney",1000.00,"2023-03-15","Jane Smith for Assembly"
"California Teachers Association","","",5000.00,"2023-03-20","Education PAC"
"Dr. Sarah Wilson","Kaiser Permanente","Physician",500.00,"2023-04-01","Healthcare Initiative"
"ACME Development LLC","","",2500.00,"2023-04-05","Housing Committee"
```

### Categorization Results
```csv
Contributor Name,Contributor Category
"John Doe","Lawyers"
"California Teachers Association","Labor Unions"
"Dr. Sarah Wilson","Individual contributor (with no other information)"
"ACME Development LLC","Real Estate Industry"
```

## Installation

### Option 1: Modern Development Installation (Recommended)
```bash
git clone https://github.com/prharms/cf-categorizer-ca-powersearch.git
cd cf-categorizer-ca-powersearch
pip install -e .
```

### Option 2: Direct Dependencies Installation
```bash
pip install -r requirements.txt
```

### Option 3: Development with Modern Tools
```bash
# Install with development dependencies
pip install -e ".[dev]"

# Or install development tools separately
pip install -e ".[dev,test]"
```

### Environment Setup
Create a `.env` file with your Anthropic API key:
```
ANTHROPIC_API_KEY=your_api_key_here

# Optional configuration overrides
API_MODEL=claude-sonnet-4-20250514
LOG_LEVEL=INFO
```

## Modern Development Features

This project uses modern Python development practices:

### **Package Configuration**
- **`pyproject.toml`**: Modern project configuration following PEP 621
- **Declarative dependencies**: No more `setup.py` or `requirements.txt` parsing
- **Optional dependency groups**: Install only what you need (`[dev]`, `[test]`)

### **Development Tools**
- **Black**: Automatic code formatting
- **isort**: Import sorting and organization
- **flake8**: Code linting and style checking
- **mypy**: Static type checking
- **pytest**: Modern testing framework with coverage

### **Development Workflow**
```bash
# Set up development environment
pip install -e ".[dev]"

# Format code
black src/ tests/
isort src/ tests/

# Run linting
flake8 src/ tests/

# Type checking
mypy src/

# Run tests with coverage
pytest --cov=src tests/
```

## Usage

### Basic Usage

```bash
# Use default file (auto-detects CSV in data/raw/)
python scripts/run_categorization.py

# Process specific PowerSearch export
python scripts/run_categorization.py data/raw/assembly_2024_contributions.csv

# Process with custom output location
python scripts/run_categorization.py my_powersearch_data.csv --output results/categorized_data.csv
```

### Advanced Usage

```bash
# Enable verbose logging for debugging
python scripts/run_categorization.py --verbose

# Custom log file location
python scripts/run_categorization.py --log-file custom_logs/processing.log

# Standardize existing categories only (skip AI categorization)
python scripts/run_categorization.py --standardize
```

### Two-Stage Pipeline

The tool runs an optimized **two-stage pipeline**:

**Stage 1 - AI Categorization:**
1. Read and validate PowerSearch CSV file
2. Process contributors using Claude 4 Sonnet API with sequential processing
3. Save raw AI categories to `data/interim/filename_categorized.csv`
4. Automatic progress saving and resume capability

**Stage 2 - Fuzzy Matching Standardization:**
5. Apply fuzzy matching to standardize category names
6. Save final results to `data/processed/filename_standardized.csv`

**Result:** Two files created following data science best practices!

## Performance & Scale

**⚡ Optimized Rate Limiting**: The tool is configured with 1.5-second delays between API calls, optimized for Claude 4 Sonnet's 50 requests/minute limit while maintaining safe margins.

### Processing Times (PowerSearch Exports)
- **Small datasets** (< 1,000 contributions): ~4-7 minutes
- **Medium datasets** (1,000-10,000 contributions): ~20-40 minutes  
- **Large datasets** (> 10,000 contributions): 45 minutes - 2 hours

### API Costs (Anthropic Claude)
- ~$0.01-0.02 per 100 contributors (varies by name/employer complexity)
- Medium PowerSearch exports (5k-10k rows): ~$1-3
- Large PowerSearch exports (50k+ rows): ~$5-15

### Recommended Workflow for Large PowerSearch Exports
1. **Test first**: Start with a small date range to verify categorization quality
2. **Monitor progress**: Enable `--verbose` to see real-time progress
3. **Leverage persistence**: Interrupted processing can resume automatically
4. **Split large exports**: For 100k+ rows, export multiple date ranges and process separately

### PowerSearch Export Size Considerations
- PowerSearch limits exports to ~200,000 rows per search
- For comprehensive datasets, export multiple date ranges and combine results
- Memory usage: ~100MB RAM per 10,000 contributors

## Categories

The tool categorizes contributors into these standardized categories:
- **Democratic Party Committees**
- **Other political action committees**
- **State Legislative Candidates/Officeholders**
- **Local Government Candidates/Officeholders**
- **Labor Unions**
- **Environmental Groups**
- **Oil Industry**
- **Pharmaceutical Industry**
- **Real Estate Industry**
- **Indian Tribes**
- **Lobbyists and Political Consultants**
- **Lawyers**
- **Individual contributor (with no other information)**
- **Business contributor (with no other information)**
- **Other**

## Troubleshooting

### PowerSearch Data Issues
- **"Extra rows at bottom"**: Remove summary/footer lines after last contribution row
- **Missing required columns**: Ensure PowerSearch export includes Contributor Name, Employer, Occupation columns
- **Empty contributor names**: PowerSearch sometimes exports committee IDs only - these will be categorized as "Other"
- **Special characters**: PowerSearch may include unicode characters that cause encoding issues on Windows

### Common Errors
- `ValidationError: Missing required columns` → Check your PowerSearch export includes required columns
- `ValidationError: CSV file must contain 'Contributor Name' column` → Verify column headers match exactly
- `UnicodeEncodeError` → Save your CSV as UTF-8 encoding (not Excel default)
- `Rate limit exceeded` → Wait and resume processing (automatic retry with backoff)
- `ANTHROPIC_API_KEY not found` → Create `.env` file with your API key

### Windows-Specific Issues
- **Emoji encoding errors**: Use `--log-file` to avoid console encoding issues
- **Path issues**: Use forward slashes in file paths: `data/raw/file.csv`
- **CSV encoding**: Open in Notepad and save as "UTF-8" if you see strange characters

### Performance Issues
- **Slow processing**: Check internet connection and API key validity
- **Memory errors**: Process files in smaller chunks (split by date range)
- **Network timeouts**: Check internet connection; processing will resume automatically

## Project Structure

Following **Modern Python Best Practices** with enhanced modular design:

```
cf-categorizer-ca-powersearch/
├── data/
│   ├── raw/                     # PowerSearch CSV exports (cleaned)
│   ├── interim/                 # Raw AI categorization results
│   └── processed/               # Final standardized categorization
├── src/
│   ├── api/                     # Anthropic API client and rate limiting
│   ├── cli/                     # Command-line interface
│   ├── config/                  # Configuration management
│   ├── processing/              # Core categorization logic
│   ├── utils/                   # Validation and logging utilities
│   └── __init__.py
├── scripts/                     # Executable entry points
│   └── run_categorization.py    # Main entry point script
├── tests/                       # Comprehensive test suite
├── logs/                        # Application logs
├── .env                         # Environment variables (API keys)
├── .gitignore                   # Git ignore patterns
├── pyproject.toml               # Modern Python project configuration
├── requirements.txt             # Python dependencies (legacy support)
└── README.md                    # This file
```

## Configuration

### Runtime Configuration
The tool supports extensive configuration through environment variables:

```bash
# API Configuration
API_MODEL=claude-sonnet-4-20250514
BATCH_SIZE=10

# Logging Configuration
LOG_LEVEL=INFO
LOG_FILE=logs/categorization.log

# Processing Configuration
FUZZY_MATCH_THRESHOLD=80
PROGRESS_SAVE_INTERVAL=50
```

### Development Configuration
Modern development tools are configured in `pyproject.toml`:

- **Black**: Code formatting (line length: 88)
- **isort**: Import sorting (black-compatible)
- **mypy**: Type checking (Python 3.8+)
- **pytest**: Test discovery and execution
- **coverage**: Test coverage reporting

All tool configurations can be found in the `[tool.*]` sections of `pyproject.toml`.

## Testing

Run the comprehensive test suite:

### Modern Testing (Recommended)
```bash
# Install test dependencies
pip install -e ".[test]"

# Run all tests with coverage
pytest --cov=src tests/

# Run specific test modules
pytest tests/test_api_client.py
pytest tests/test_processing.py
pytest tests/test_config.py
pytest tests/test_validation.py

# Generate coverage report
pytest --cov=src --cov-report=html tests/
```

### Legacy Testing
```bash
# Run all tests (legacy test runner)
python tests/run_all_tests.py

# Run specific test modules (legacy unittest)
python -m unittest tests.test_api_client
python -m unittest tests.test_processing
python -m unittest tests.test_config
python -m unittest tests.test_validation
```

## Dependencies

### Runtime Dependencies
- **pandas**: For CSV file processing and data manipulation
- **anthropic**: For Claude API integration and AI categorization
- **python-dotenv**: For environment variable management
- **fuzzywuzzy**: For fuzzy string matching and category standardization
- **python-levenshtein**: For fast string similarity calculations
- **dataclasses-json**: For configuration serialization
- **typing-extensions**: For enhanced type hints

### Development Dependencies (Optional)
- **pytest**: For comprehensive testing
- **pytest-cov**: For test coverage reporting
- **black**: For code formatting
- **isort**: For import sorting
- **flake8**: For linting
- **mypy**: For type checking
- **pre-commit**: For git hooks

Install development dependencies with:
```bash
pip install -e ".[dev]"
```

## Data Flow

Enhanced data transformation pipeline optimized for PowerSearch exports:

```
PowerSearch Export  →    Cleaned CSV       →    AI Categorization    →    Standardized Results
(with footer junk)      (manual cleanup)       (raw categories)          (fuzzy-matched clean)
      ↓                       ↓                        ↓                         ↓
Manual cleanup         Input validation        Sequential processing    Quality validation
Remove footers        Check required cols     Progress persistence     Category normalization
Save to data/raw/     Error handling         Optimized rate limiting  Save to data/processed/
```

## Error Handling

The tool includes comprehensive error handling for PowerSearch data:

- **Input validation**: Validates CSV format and required PowerSearch columns
- **API error recovery**: Graceful handling of rate limits and failures with automatic retry
- **File system errors**: Proper handling of permission and disk issues
- **Progress persistence**: Automatic recovery from interruptions using checkpoint files
- **PowerSearch format issues**: Handles common export format variations
- **Detailed error reporting**: Clear error messages with context and suggested fixes

## Contributing

1. Fork the repository
2. Create a feature branch for PowerSearch improvements
3. Add comprehensive tests for new features
4. Ensure all tests pass: `python tests/run_all_tests.py`
5. Test with real PowerSearch exports
6. Submit a pull request

## License

MIT License - see LICENSE file for details.

## Support

For issues with:
- **PowerSearch exports**: Check the troubleshooting section above
- **Tool functionality**: Open a GitHub issue with sample data (anonymized)
- **API costs**: Monitor usage at [Anthropic Console](https://console.anthropic.com)

## Modern Python Migration

This project has been migrated to modern Python packaging standards:

- **✅ `pyproject.toml`**: Modern project configuration (PEP 621)
- **✅ Declarative dependencies**: No more `setup.py` complexity
- **✅ Development tools**: Black, isort, flake8, mypy, pytest
- **✅ Optional dependencies**: Install only what you need
- **✅ Backwards compatibility**: `requirements.txt` still works

The migration maintains full compatibility with existing workflows while providing modern development tools and practices.

---

**Note**: This tool is not affiliated with the California Secretary of State. It is designed to work with publicly available campaign finance data from their PowerSearch portal. 